{"cells":[{"cell_type":"code","source":["print(sc.version)\nprint(sc.pythonVer)\nprint(sc.master)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b23c148-78bf-4812-8583-4c18f1c550c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">3.1.1\n3.8\nlocal[8]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">3.1.1\n3.8\nlocal[8]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["numb = range(1, 100)\nspark_data = sc.parallelize(numb)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3367214-2a5c-49b2-bfb9-fb9448a6d4d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["textfile = sc.textFile(\"README.md\")\nprint(textfile)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ace3e51-b5ed-42db-97a9-cfc2c38c9674"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">README.md MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">README.md MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["my_list = [1,2,3,4,5]\nmapping = list(map(lambda x: x * 2 , my_list))\nprint(mapping)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41b18b00-7bd7-4fa5-acfe-3a8a17f350a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[2, 4, 6, 8, 10]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[2, 4, 6, 8, 10]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["my_list2 = [10,15,20,25,30,35]\nfiltering = list(filter(lambda x: (x%10 == 0), my_list2))\nprint(filtering)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b3c066-7c4b-4ae0-abc7-0a344dbcffe3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[10, 20, 30]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[10, 20, 30]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\nprint(\"The type of RDD is\", type(RDD))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91da049d-a012-4742-8b29-cea49d1bc611"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The type of RDD is &lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The type of RDD is &lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_path = \"/FileStore/tables/Readme.md\"\nprint(\"The file_path is\", file_path)\nfileRDD = sc.textFile(file_path)\nprint(\"The file type of fileRDD is\", type(fileRDD))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7215b096-8e10-4a3e-99a0-6a7954d62b65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The file_path is /FileStore/tables/Readme.md\nThe file type of fileRDD is &lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The file_path is /FileStore/tables/Readme.md\nThe file type of fileRDD is &lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\nfileRDD_part = sc.textFile(file_path, minPartitions = 5)\nprint(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b298900-ba68-4067-8076-2780762d629d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Number of partitions in fileRDD is 2\nNumber of partitions in fileRDD_part is 5\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of partitions in fileRDD is 2\nNumber of partitions in fileRDD_part is 5\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["num_RDD = sc.parallelize([1,2,3,4])\ncubedRDD = num_RDD.map(lambda x: x * x * x)\nnumbers_all = cubedRDD.collect()\nprint(numbers_all)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dccfe3e8-20f8-40c2-8354-b66538b32e63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[1, 8, 27, 64]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[1, 8, 27, 64]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["fileRDD = sc.textFile(\"/FileStore/tables/spark.txt\")\nfileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\nprint(fileRDD_filter.count())\nfor line in fileRDD_filter.take(4):\n  print(line)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94c944e4-6b94-40a9-a355-9f020b0cdedf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">4\nsparkSince its release, Apache Spark, the unified analytics engine, has seen rapid adoption by enterprises across a wide range of industries. \nInternet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. \nThe team that started the Spark research project at UC Berkeley founded Databricks in 2013.\nApache Spark is a lightning-fast unified analytics engine for big data and machine learning. \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">4\nsparkSince its release, Apache Spark, the unified analytics engine, has seen rapid adoption by enterprises across a wide range of industries. \nInternet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. \nThe team that started the Spark research project at UC Berkeley founded Databricks in 2013.\nApache Spark is a lightning-fast unified analytics engine for big data and machine learning. \n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\nRdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\nfor num in Rdd_Reduced.collect():\n    print(\"Key {} has {} Counts\".format(num[0], num[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89936ac6-4dca-4a68-a1c9-c29b21e85b97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Key 1 has 2 Counts\nKey 3 has 10 Counts\nKey 4 has 5 Counts\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Key 1 has 2 Counts\nKey 3 has 10 Counts\nKey 4 has 5 Counts\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\nfor num in Rdd_Reduced_Sort.collect():\n  print(\"Key {} has {} Counts\".format(num[0], num[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dc536eb-25e2-4efb-a0f8-1ac8079f3922"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Key 4 has 5 Counts\nKey 3 has 10 Counts\nKey 1 has 2 Counts\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Key 4 has 5 Counts\nKey 3 has 10 Counts\nKey 1 has 2 Counts\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["total = Rdd.countByKey()\nprint(\"The type of total is\", type(total))\nfor k, v in total.items():\n  print(\"key\", k, \"has\", v, \"counts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d93a72cb-580e-45cf-b093-e0919a970131"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The type of total is &lt;class &#39;collections.defaultdict&#39;&gt;\nkey 1 has 1 counts\nkey 3 has 2 counts\nkey 4 has 1 counts\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The type of total is &lt;class &#39;collections.defaultdict&#39;&gt;\nkey 1 has 1 counts\nkey 3 has 2 counts\nkey 4 has 1 counts\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["baseRDD = sc.textFile(file_path)\nsplitRDD = baseRDD.flatMap(lambda x: x.split())\nprint(\"Total number of words in splitRDD:\", splitRDD.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e8d120b-3bbf-4fef-a34f-be2818ec352f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Total number of words in splitRDD: 44\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total number of words in splitRDD: 44\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["stop_words = [',','.',';',':']\nsplitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\nsplitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\nresultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e01792ce-86a4-493c-90d4-56c44a8b534f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["for word in resultRDD.take(10):\n  print(word)\n\nresultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n\nresultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n\nfor word in resultRDD_swap_sort.take(10):\n  print(\"{} has {} counts\". format(word[1], word[0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4b61128-28c6-4d9b-b930-b43fec9498dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">(&#39;#&#39;, 1)\n(&#39;Analyzing&#39;, 2)\n(&#39;Cleveland&#39;, 2)\n(&#39;Disease&#39;, 2)\n(&#39;Dataset.rmd&#39;, 1)\n(&#39;source&#39;, 1)\n(&#39;Mathematical&#39;, 1)\n(&#39;modeling.&#39;, 1)\n(&#39;heart.csv&#39;, 1)\n(&#39;is&#39;, 2)\nthe has 4 counts\nfor has 3 counts\ndata has 3 counts\nAnalyzing has 2 counts\nCleveland has 2 counts\nDisease has 2 counts\nis has 2 counts\nHeart has 2 counts\n# has 1 counts\nDataset.rmd has 1 counts\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;#&#39;, 1)\n(&#39;Analyzing&#39;, 2)\n(&#39;Cleveland&#39;, 2)\n(&#39;Disease&#39;, 2)\n(&#39;Dataset.rmd&#39;, 1)\n(&#39;source&#39;, 1)\n(&#39;Mathematical&#39;, 1)\n(&#39;modeling.&#39;, 1)\n(&#39;heart.csv&#39;, 1)\n(&#39;is&#39;, 2)\nthe has 4 counts\nfor has 3 counts\ndata has 3 counts\nAnalyzing has 2 counts\nCleveland has 2 counts\nDisease has 2 counts\nis has 2 counts\nHeart has 2 counts\n# has 1 counts\nDataset.rmd has 1 counts\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\nrdd = sc.parallelize(sample_list)\nnames_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\nprint(\"The type of names_df is\", type(names_df))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dafee0fa-72c9-41d6-8e2c-ab76ced21c00"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The type of names_df is &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The type of names_df is &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_path = \"/FileStore/tables/people.csv\"\npeople_df = spark.read.csv(file_path, header=True, inferSchema=True)\nprint(\"The type of people_df is\", type(people_df))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e73b558e-5b80-4cd2-830a-454fe7c2ebd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The type of people_df is &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The type of people_df is &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["people_df.show(10)\nprint(people_df.count())\nprint(len(people_df.columns))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4d418c7-5d28-47ed-b446-d8e313173f86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---+---------+----------------+------+-------------+\n|_c0|person_id|            name|   sex|date of birth|\n+---+---------+----------------+------+-------------+\n|  0|      100|  Penelope Lewis|female|   1990-08-31|\n|  1|      101|   David Anthony|  male|   1971-10-14|\n|  2|      102|       Ida Shipp|female|   1962-05-24|\n|  3|      103|    Joanna Moore|female|   2017-03-10|\n|  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n|  5|      105|   David Simmons|  male|   1999-12-30|\n|  6|      106|   Edward Hudson|  male|   1983-05-09|\n|  7|      107|    Albert Jones|  male|   1990-09-13|\n|  8|      108|Leonard Cavender|  male|   1958-08-08|\n|  9|      109|  Everett Vadala|  male|   2005-05-24|\n+---+---------+----------------+------+-------------+\nonly showing top 10 rows\n\n100000\n5\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---------+----------------+------+-------------+\n_c0|person_id|            name|   sex|date of birth|\n+---+---------+----------------+------+-------------+\n  0|      100|  Penelope Lewis|female|   1990-08-31|\n  1|      101|   David Anthony|  male|   1971-10-14|\n  2|      102|       Ida Shipp|female|   1962-05-24|\n  3|      103|    Joanna Moore|female|   2017-03-10|\n  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n  5|      105|   David Simmons|  male|   1999-12-30|\n  6|      106|   Edward Hudson|  male|   1983-05-09|\n  7|      107|    Albert Jones|  male|   1990-09-13|\n  8|      108|Leonard Cavender|  male|   1958-08-08|\n  9|      109|  Everett Vadala|  male|   2005-05-24|\n+---+---------+----------------+------+-------------+\nonly showing top 10 rows\n\n100000\n5\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["people_df_sub = people_df.select(\"name\",\"sex\",\"date of birth\")\npeople_df_sub.show(10)\npeople_df_sub_nodup = people_df_sub.dropDuplicates()\nprint(people_df_sub.count())\nprint(people_df_sub_nodup.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b01518e-941c-4a37-a84d-79cb33bfcd68"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------------+------+-------------+\n|            name|   sex|date of birth|\n+----------------+------+-------------+\n|  Penelope Lewis|female|   1990-08-31|\n|   David Anthony|  male|   1971-10-14|\n|       Ida Shipp|female|   1962-05-24|\n|    Joanna Moore|female|   2017-03-10|\n|  Lisandra Ortiz|female|   2020-08-05|\n|   David Simmons|  male|   1999-12-30|\n|   Edward Hudson|  male|   1983-05-09|\n|    Albert Jones|  male|   1990-09-13|\n|Leonard Cavender|  male|   1958-08-08|\n|  Everett Vadala|  male|   2005-05-24|\n+----------------+------+-------------+\nonly showing top 10 rows\n\n100000\n99998\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+------+-------------+\n            name|   sex|date of birth|\n+----------------+------+-------------+\n  Penelope Lewis|female|   1990-08-31|\n   David Anthony|  male|   1971-10-14|\n       Ida Shipp|female|   1962-05-24|\n    Joanna Moore|female|   2017-03-10|\n  Lisandra Ortiz|female|   2020-08-05|\n   David Simmons|  male|   1999-12-30|\n   Edward Hudson|  male|   1983-05-09|\n    Albert Jones|  male|   1990-09-13|\nLeonard Cavender|  male|   1958-08-08|\n  Everett Vadala|  male|   2005-05-24|\n+----------------+------+-------------+\nonly showing top 10 rows\n\n100000\n99998\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["people_df_female = people_df.filter(people_df.sex == \"female\")\npeople_df_male = people_df.filter(people_df.sex == \"male\")\nprint(\"Female count : \",people_df_female.count())\nprint(\"Male count : \",people_df_male.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3450db3-b350-4a18-9257-6118a868b14a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Female count :  49014\nMale count :  49066\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Female count :  49014\nMale count :  49066\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["people_df.createOrReplaceTempView(\"people\")\npeople_df_names = spark.sql(\"SELECT name FROM people\")\npeople_df_names.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2259917c-f302-4366-92a9-0d8151ef6309"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------------+\n|            name|\n+----------------+\n|  Penelope Lewis|\n|   David Anthony|\n|       Ida Shipp|\n|    Joanna Moore|\n|  Lisandra Ortiz|\n|   David Simmons|\n|   Edward Hudson|\n|    Albert Jones|\n|Leonard Cavender|\n|  Everett Vadala|\n+----------------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\n            name|\n+----------------+\n  Penelope Lewis|\n   David Anthony|\n       Ida Shipp|\n    Joanna Moore|\n  Lisandra Ortiz|\n   David Simmons|\n   Edward Hudson|\n    Albert Jones|\nLeonard Cavender|\n  Everett Vadala|\n+----------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["people_df_female = spark.sql(\"SELECT * from people WHERE sex= 'female'\")\npeople_df_male =  spark.sql(\"SELECT * from people WHERE sex= 'male'\")\nprint(\"Female count : \",people_df_female.count())\nprint(\"Male count : \",people_df_male.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac5b8f84-b0f8-4a1b-b4c9-c79149997fd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Female count :  49014\nMale count :  49066\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Female count :  49014\nMale count :  49066\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(names_df.columns)\ndf_pandas = names_df.toPandas()\ndf_pandas.plot(kind='barh', x='Name', y='Age',colormap='winter_r')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"706c1a8f-e505-4b0d-b846-18ca4af978bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;Name&#39;, &#39;Age&#39;]\nOut[34]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Name&#39;, &#39;Age&#39;]\nOut[34]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/6d7940ed-5cd1-4ead-b14b-a25e2daa24a0.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3af823e160&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3af823e160&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_path = \"/FileStore/tables/Fifa2018_dataset.csv\"\nfifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\nfifa_df.printSchema()\nfifa_df.show(10)\nprint(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e5b0fe1-47da-41d6-891b-0d502e7e7f66"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Photo: string (nullable = true)\n |-- Nationality: string (nullable = true)\n |-- Flag: string (nullable = true)\n |-- Overall: integer (nullable = true)\n |-- Potential: integer (nullable = true)\n |-- Club: string (nullable = true)\n |-- Club Logo: string (nullable = true)\n |-- Value: string (nullable = true)\n |-- Wage: string (nullable = true)\n |-- Special: integer (nullable = true)\n |-- Acceleration: string (nullable = true)\n |-- Aggression: string (nullable = true)\n |-- Agility: string (nullable = true)\n |-- Balance: string (nullable = true)\n |-- Ball control: string (nullable = true)\n |-- Composure: string (nullable = true)\n |-- Crossing: string (nullable = true)\n |-- Curve: string (nullable = true)\n |-- Dribbling: string (nullable = true)\n |-- Finishing: string (nullable = true)\n |-- Free kick accuracy: string (nullable = true)\n |-- GK diving: string (nullable = true)\n |-- GK handling: string (nullable = true)\n |-- GK kicking: string (nullable = true)\n |-- GK positioning: string (nullable = true)\n |-- GK reflexes: string (nullable = true)\n |-- Heading accuracy: string (nullable = true)\n |-- Interceptions: string (nullable = true)\n |-- Jumping: string (nullable = true)\n |-- Long passing: string (nullable = true)\n |-- Long shots: string (nullable = true)\n |-- Marking: string (nullable = true)\n |-- Penalties: string (nullable = true)\n |-- Positioning: string (nullable = true)\n |-- Reactions: string (nullable = true)\n |-- Short passing: string (nullable = true)\n |-- Shot power: string (nullable = true)\n |-- Sliding tackle: string (nullable = true)\n |-- Sprint speed: string (nullable = true)\n |-- Stamina: string (nullable = true)\n |-- Standing tackle: string (nullable = true)\n |-- Strength: string (nullable = true)\n |-- Vision: string (nullable = true)\n |-- Volleys: string (nullable = true)\n |-- CAM: double (nullable = true)\n |-- CB: double (nullable = true)\n |-- CDM: double (nullable = true)\n |-- CF: double (nullable = true)\n |-- CM: double (nullable = true)\n |-- ID: integer (nullable = true)\n |-- LAM: double (nullable = true)\n |-- LB: double (nullable = true)\n |-- LCB: double (nullable = true)\n |-- LCM: double (nullable = true)\n |-- LDM: double (nullable = true)\n |-- LF: double (nullable = true)\n |-- LM: double (nullable = true)\n |-- LS: double (nullable = true)\n |-- LW: double (nullable = true)\n |-- LWB: double (nullable = true)\n |-- Preferred Positions: string (nullable = true)\n |-- RAM: double (nullable = true)\n |-- RB: double (nullable = true)\n |-- RCB: double (nullable = true)\n |-- RCM: double (nullable = true)\n |-- RDM: double (nullable = true)\n |-- RF: double (nullable = true)\n |-- RM: double (nullable = true)\n |-- RS: double (nullable = true)\n |-- RW: double (nullable = true)\n |-- RWB: double (nullable = true)\n |-- ST: double (nullable = true)\n\n+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n|_c0|             Name|Age|               Photo|Nationality|                Flag|Overall|Potential|               Club|           Club Logo| Value| Wage|Special|Acceleration|Aggression|Agility|Balance|Ball control|Composure|Crossing|Curve|Dribbling|Finishing|Free kick accuracy|GK diving|GK handling|GK kicking|GK positioning|GK reflexes|Heading accuracy|Interceptions|Jumping|Long passing|Long shots|Marking|Penalties|Positioning|Reactions|Short passing|Shot power|Sliding tackle|Sprint speed|Stamina|Standing tackle|Strength|Vision|Volleys| CAM|  CB| CDM|  CF|  CM|    ID| LAM|  LB| LCB| LCM| LDM|  LF|  LM|  LS|  LW| LWB|Preferred Positions| RAM|  RB| RCB| RCM| RDM|  RF|  RM|  RS|  RW| RWB|  ST|\n+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n|  0|Cristiano Ronaldo| 32|https://cdn.sofif...|   Portugal|https://cdn.sofif...|     94|       94|     Real Madrid CF|https://cdn.sofif...|€95.5M|€565K|   2228|          89|        63|     89|     63|          93|       95|      85|   81|       91|       94|                76|        7|         11|        15|            14|         11|              88|           29|     95|          77|        92|     22|       85|         95|       96|           83|        94|            23|          91|     92|             31|      80|    85|     88|89.0|53.0|62.0|91.0|82.0| 20801|89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|             ST LW |89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|92.0|\n|  1|         L. Messi| 30|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     93|       93|       FC Barcelona|https://cdn.sofif...| €105M|€565K|   2154|          92|        48|     90|     95|          95|       96|      77|   89|       97|       95|                90|        6|         11|        15|            14|          8|              71|           22|     68|          87|        88|     13|       74|         93|       95|           88|        85|            26|          87|     73|             28|      59|    90|     85|92.0|45.0|59.0|92.0|84.0|158023|92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|                RW |92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|88.0|\n|  2|           Neymar| 25|https://cdn.sofif...|     Brazil|https://cdn.sofif...|     92|       94|Paris Saint-Germain|https://cdn.sofif...| €123M|€280K|   2100|          94|        56|     96|     82|          95|       92|      75|   81|       96|       89|                84|        9|          9|        15|            15|         11|              62|           36|     61|          75|        77|     21|       81|         90|       88|           81|        80|            33|          90|     78|             24|      53|    80|     83|88.0|46.0|59.0|88.0|79.0|190871|88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|                LW |88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|84.0|\n|  3|        L. Suárez| 30|https://cdn.sofif...|    Uruguay|https://cdn.sofif...|     92|       92|       FC Barcelona|https://cdn.sofif...|  €97M|€510K|   2291|          88|        78|     86|     60|          91|       83|      77|   86|       86|       94|                84|       27|         25|        31|            33|         37|              77|           41|     69|          64|        86|     30|       85|         92|       93|           83|        87|            38|          77|     89|             45|      80|    84|     88|87.0|58.0|65.0|88.0|80.0|176580|87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|                ST |87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|88.0|\n|  4|         M. Neuer| 31|https://cdn.sofif...|    Germany|https://cdn.sofif...|     92|       92|   FC Bayern Munich|https://cdn.sofif...|  €61M|€230K|   1493|          58|        29|     52|     35|          48|       70|      15|   14|       30|       13|                11|       91|         90|        95|            91|         89|              25|           30|     78|          59|        16|     10|       47|         12|       85|           55|        25|            11|          61|     44|             10|      83|    70|     11|null|null|null|null|null|167495|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n|  5|   R. Lewandowski| 28|https://cdn.sofif...|     Poland|https://cdn.sofif...|     91|       91|   FC Bayern Munich|https://cdn.sofif...|  €92M|€355K|   2143|          79|        80|     78|     80|          89|       87|      62|   77|       85|       91|                84|       15|          6|        12|             8|         10|              85|           39|     84|          65|        83|     25|       81|         91|       91|           83|        88|            19|          83|     79|             42|      84|    78|     87|84.0|57.0|62.0|87.0|78.0|188545|84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|                ST |84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|88.0|\n|  6|           De Gea| 26|https://cdn.sofif...|      Spain|https://cdn.sofif...|     90|       92|  Manchester United|https://cdn.sofif...|€64.5M|€215K|   1458|          57|        38|     60|     43|          42|       64|      17|   21|       18|       13|                19|       90|         85|        87|            86|         90|              21|           30|     67|          51|        12|     13|       40|         12|       88|           50|        31|            13|          58|     40|             21|      64|    68|     13|null|null|null|null|null|193080|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n|  7|        E. Hazard| 26|https://cdn.sofif...|    Belgium|https://cdn.sofif...|     90|       91|            Chelsea|https://cdn.sofif...|€90.5M|€295K|   2096|          93|        54|     93|     91|          92|       87|      80|   82|       93|       83|                79|       11|         12|         6|             8|          8|              57|           41|     59|          81|        82|     25|       86|         85|       85|           86|        79|            22|          87|     79|             27|      65|    86|     79|88.0|47.0|61.0|87.0|81.0|183277|88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|                LW |88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|82.0|\n|  8|         T. Kroos| 27|https://cdn.sofif...|    Germany|https://cdn.sofif...|     90|       90|     Real Madrid CF|https://cdn.sofif...|  €79M|€340K|   2165|          60|        60|     71|     69|          89|       85|      85|   85|       79|       76|                84|       10|         11|        13|             7|         10|              54|           85|     32|          93|        90|     63|       73|         79|       86|           90|        87|            69|          52|     77|             82|      74|    88|     82|83.0|72.0|82.0|81.0|87.0|182521|83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|            CDM CM |83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|77.0|\n|  9|       G. Higuaín| 29|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     90|       90|           Juventus|https://cdn.sofif...|  €77M|€275K|   1961|          78|        50|     75|     69|          85|       86|      68|   74|       84|       91|                62|        5|         12|         7|             5|         10|              86|           20|     79|          59|        82|     12|       70|         92|       88|           75|        88|            18|          80|     72|             22|      85|    70|     88|81.0|46.0|52.0|84.0|71.0|167664|81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|                ST |81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|87.0|\n+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\nonly showing top 10 rows\n\nThere are 17981 rows in the fifa_df DataFrame\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- Name: string (nullable = true)\n-- Age: integer (nullable = true)\n-- Photo: string (nullable = true)\n-- Nationality: string (nullable = true)\n-- Flag: string (nullable = true)\n-- Overall: integer (nullable = true)\n-- Potential: integer (nullable = true)\n-- Club: string (nullable = true)\n-- Club Logo: string (nullable = true)\n-- Value: string (nullable = true)\n-- Wage: string (nullable = true)\n-- Special: integer (nullable = true)\n-- Acceleration: string (nullable = true)\n-- Aggression: string (nullable = true)\n-- Agility: string (nullable = true)\n-- Balance: string (nullable = true)\n-- Ball control: string (nullable = true)\n-- Composure: string (nullable = true)\n-- Crossing: string (nullable = true)\n-- Curve: string (nullable = true)\n-- Dribbling: string (nullable = true)\n-- Finishing: string (nullable = true)\n-- Free kick accuracy: string (nullable = true)\n-- GK diving: string (nullable = true)\n-- GK handling: string (nullable = true)\n-- GK kicking: string (nullable = true)\n-- GK positioning: string (nullable = true)\n-- GK reflexes: string (nullable = true)\n-- Heading accuracy: string (nullable = true)\n-- Interceptions: string (nullable = true)\n-- Jumping: string (nullable = true)\n-- Long passing: string (nullable = true)\n-- Long shots: string (nullable = true)\n-- Marking: string (nullable = true)\n-- Penalties: string (nullable = true)\n-- Positioning: string (nullable = true)\n-- Reactions: string (nullable = true)\n-- Short passing: string (nullable = true)\n-- Shot power: string (nullable = true)\n-- Sliding tackle: string (nullable = true)\n-- Sprint speed: string (nullable = true)\n-- Stamina: string (nullable = true)\n-- Standing tackle: string (nullable = true)\n-- Strength: string (nullable = true)\n-- Vision: string (nullable = true)\n-- Volleys: string (nullable = true)\n-- CAM: double (nullable = true)\n-- CB: double (nullable = true)\n-- CDM: double (nullable = true)\n-- CF: double (nullable = true)\n-- CM: double (nullable = true)\n-- ID: integer (nullable = true)\n-- LAM: double (nullable = true)\n-- LB: double (nullable = true)\n-- LCB: double (nullable = true)\n-- LCM: double (nullable = true)\n-- LDM: double (nullable = true)\n-- LF: double (nullable = true)\n-- LM: double (nullable = true)\n-- LS: double (nullable = true)\n-- LW: double (nullable = true)\n-- LWB: double (nullable = true)\n-- Preferred Positions: string (nullable = true)\n-- RAM: double (nullable = true)\n-- RB: double (nullable = true)\n-- RCB: double (nullable = true)\n-- RCM: double (nullable = true)\n-- RDM: double (nullable = true)\n-- RF: double (nullable = true)\n-- RM: double (nullable = true)\n-- RS: double (nullable = true)\n-- RW: double (nullable = true)\n-- RWB: double (nullable = true)\n-- ST: double (nullable = true)\n\n+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n_c0|             Name|Age|               Photo|Nationality|                Flag|Overall|Potential|               Club|           Club Logo| Value| Wage|Special|Acceleration|Aggression|Agility|Balance|Ball control|Composure|Crossing|Curve|Dribbling|Finishing|Free kick accuracy|GK diving|GK handling|GK kicking|GK positioning|GK reflexes|Heading accuracy|Interceptions|Jumping|Long passing|Long shots|Marking|Penalties|Positioning|Reactions|Short passing|Shot power|Sliding tackle|Sprint speed|Stamina|Standing tackle|Strength|Vision|Volleys| CAM|  CB| CDM|  CF|  CM|    ID| LAM|  LB| LCB| LCM| LDM|  LF|  LM|  LS|  LW| LWB|Preferred Positions| RAM|  RB| RCB| RCM| RDM|  RF|  RM|  RS|  RW| RWB|  ST|\n+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n  0|Cristiano Ronaldo| 32|https://cdn.sofif...|   Portugal|https://cdn.sofif...|     94|       94|     Real Madrid CF|https://cdn.sofif...|€95.5M|€565K|   2228|          89|        63|     89|     63|          93|       95|      85|   81|       91|       94|                76|        7|         11|        15|            14|         11|              88|           29|     95|          77|        92|     22|       85|         95|       96|           83|        94|            23|          91|     92|             31|      80|    85|     88|89.0|53.0|62.0|91.0|82.0| 20801|89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|             ST LW |89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|92.0|\n  1|         L. Messi| 30|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     93|       93|       FC Barcelona|https://cdn.sofif...| €105M|€565K|   2154|          92|        48|     90|     95|          95|       96|      77|   89|       97|       95|                90|        6|         11|        15|            14|          8|              71|           22|     68|          87|        88|     13|       74|         93|       95|           88|        85|            26|          87|     73|             28|      59|    90|     85|92.0|45.0|59.0|92.0|84.0|158023|92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|                RW |92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|88.0|\n  2|           Neymar| 25|https://cdn.sofif...|     Brazil|https://cdn.sofif...|     92|       94|Paris Saint-Germain|https://cdn.sofif...| €123M|€280K|   2100|          94|        56|     96|     82|          95|       92|      75|   81|       96|       89|                84|        9|          9|        15|            15|         11|              62|           36|     61|          75|        77|     21|       81|         90|       88|           81|        80|            33|          90|     78|             24|      53|    80|     83|88.0|46.0|59.0|88.0|79.0|190871|88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|                LW |88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|84.0|\n  3|        L. Suárez| 30|https://cdn.sofif...|    Uruguay|https://cdn.sofif...|     92|       92|       FC Barcelona|https://cdn.sofif...|  €97M|€510K|   2291|          88|        78|     86|     60|          91|       83|      77|   86|       86|       94|                84|       27|         25|        31|            33|         37|              77|           41|     69|          64|        86|     30|       85|         92|       93|           83|        87|            38|          77|     89|             45|      80|    84|     88|87.0|58.0|65.0|88.0|80.0|176580|87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|                ST |87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|88.0|\n  4|         M. Neuer| 31|https://cdn.sofif...|    Germany|https://cdn.sofif...|     92|       92|   FC Bayern Munich|https://cdn.sofif...|  €61M|€230K|   1493|          58|        29|     52|     35|          48|       70|      15|   14|       30|       13|                11|       91|         90|        95|            91|         89|              25|           30|     78|          59|        16|     10|       47|         12|       85|           55|        25|            11|          61|     44|             10|      83|    70|     11|null|null|null|null|null|167495|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n  5|   R. Lewandowski| 28|https://cdn.sofif...|     Poland|https://cdn.sofif...|     91|       91|   FC Bayern Munich|https://cdn.sofif...|  €92M|€355K|   2143|          79|        80|     78|     80|          89|       87|      62|   77|       85|       91|                84|       15|          6|        12|             8|         10|              85|           39|     84|          65|        83|     25|       81|         91|       91|           83|        88|            19|          83|     79|             42|      84|    78|     87|84.0|57.0|62.0|87.0|78.0|188545|84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|                ST |84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|88.0|\n  6|           De Gea| 26|https://cdn.sofif...|      Spain|https://cdn.sofif...|     90|       92|  Manchester United|https://cdn.sofif...|€64.5M|€215K|   1458|          57|        38|     60|     43|          42|       64|      17|   21|       18|       13|                19|       90|         85|        87|            86|         90|              21|           30|     67|          51|        12|     13|       40|         12|       88|           50|        31|            13|          58|     40|             21|      64|    68|     13|null|null|null|null|null|193080|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n  7|        E. Hazard| 26|https://cdn.sofif...|    Belgium|https://cdn.sofif...|     90|       91|            Chelsea|https://cdn.sofif...|€90.5M|€295K|   2096|          93|        54|     93|     91|          92|       87|      80|   82|       93|       83|                79|       11|         12|         6|             8|          8|              57|           41|     59|          81|        82|     25|       86|         85|       85|           86|        79|            22|          87|     79|             27|      65|    86|     79|88.0|47.0|61.0|87.0|81.0|183277|88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|                LW |88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|82.0|\n  8|         T. Kroos| 27|https://cdn.sofif...|    Germany|https://cdn.sofif...|     90|       90|     Real Madrid CF|https://cdn.sofif...|  €79M|€340K|   2165|          60|        60|     71|     69|          89|       85|      85|   85|       79|       76|                84|       10|         11|        13|             7|         10|              54|           85|     32|          93|        90|     63|       73|         79|       86|           90|        87|            69|          52|     77|             82|      74|    88|     82|83.0|72.0|82.0|81.0|87.0|182521|83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|            CDM CM |83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|77.0|\n  9|       G. Higuaín| 29|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     90|       90|           Juventus|https://cdn.sofif...|  €77M|€275K|   1961|          78|        50|     75|     69|          85|       86|      68|   74|       84|       91|                62|        5|         12|         7|             5|         10|              86|           20|     79|          59|        82|     12|       70|         92|       88|           75|        88|            18|          80|     72|             22|      85|    70|     88|81.0|46.0|52.0|84.0|71.0|167664|81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|                ST |81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|87.0|\n+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\nonly showing top 10 rows\n\nThere are 17981 rows in the fifa_df DataFrame\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["fifa_df.createOrReplaceTempView('fifa_df_table')\nquery = \"SELECT Age FROM fifa_df_table WHERE Nationality == 'Germany'\"\nfifa_df_germany_age = spark.sql(query)\nfifa_df_germany_age.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17e17386-e4d2-4830-9076-604e439d767c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-----------------+\n|summary|              Age|\n+-------+-----------------+\n|  count|             1140|\n|   mean|24.20263157894737|\n| stddev|4.197096712293752|\n|    min|               16|\n|    max|               36|\n+-------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------------+\nsummary|              Age|\n+-------+-----------------+\n  count|             1140|\n   mean|24.20263157894737|\n stddev|4.197096712293752|\n    min|               16|\n    max|               36|\n+-------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\nfifa_df_germany_age_pandas.plot(kind='density')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db2c588f-4027-4c06-800d-e1715dfe7084"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[37]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/e3dfe35c-b186-45ec-932d-8739b961937b.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3af6938460&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3af6938460&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS\nfrom pyspark.mllib.clustering import KMeans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3e83e61-aaa2-4b9d-86bb-ea350ef28b2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_path = \"/FileStore/tables/ratings.csv\"\ndata = sc.textFile(file_path)\nratings = data.map(lambda l: l.split(','))\nratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\ntraining_data, test_data = ratings_final.randomSplit([0.8, 0.2])\ntype(training_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"833e1d72-3f2b-4bc9-afcf-f00c58373167"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[56]: pyspark.rdd.PipelinedRDD</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[56]: pyspark.rdd.PipelinedRDD</div>"]}}],"execution_count":0},{"cell_type":"code","source":["model = ALS.train(training_data,rank=10,iterations=10)\ntestdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\npredictions = model.predictAll(testdata_no_rating)\npredictions.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bce75b0-70aa-4927-9b00-b98ef91c4f73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424004&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>model <span class=\"ansi-blue-fg\">=</span> ALS<span class=\"ansi-blue-fg\">.</span>train<span class=\"ansi-blue-fg\">(</span>training_data<span class=\"ansi-blue-fg\">,</span>rank<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">,</span>iterations<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> testdata_no_rating <span class=\"ansi-blue-fg\">=</span> test_data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> p<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>p<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> predictions <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>predictAll<span class=\"ansi-blue-fg\">(</span>testdata_no_rating<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> predictions<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/mllib/recommendation.py</span> in <span class=\"ansi-cyan-fg\">train</span><span class=\"ansi-blue-fg\">(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    278</span>             <span class=\"ansi-blue-fg\">(</span>default<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    279</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 280</span><span class=\"ansi-red-fg\">         model = callMLlibFunc(&#34;trainALSModel&#34;, cls._prepare(ratings), rank, iterations,\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    281</span>                               lambda_, blocks, nonnegative, seed)\n<span class=\"ansi-green-intense-fg ansi-bold\">    282</span>         <span class=\"ansi-green-fg\">return</span> MatrixFactorizationModel<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/mllib/recommendation.py</span> in <span class=\"ansi-cyan-fg\">_prepare</span><span class=\"ansi-blue-fg\">(cls, ratings)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    232</span>             raise TypeError(&#34;Ratings should be represented by either an RDD or a DataFrame, &#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    233</span>                             &#34;but got %s.&#34; % type(ratings))\n<span class=\"ansi-green-fg\">--&gt; 234</span><span class=\"ansi-red-fg\">         </span>first <span class=\"ansi-blue-fg\">=</span> ratings<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    235</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>first<span class=\"ansi-blue-fg\">,</span> Rating<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    236</span>             <span class=\"ansi-green-fg\">pass</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1619</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1620</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1621</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1622</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1623</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1599</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1600</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1601</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1602</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1603</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1339</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1340</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1341</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1342</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 147) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;Rating&#39; is not defined&#39;, from &lt;command-1735170817424003&gt;, line 4. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/rddsampler.py&#34;, line 95, in func\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424003&gt;&#34;, line 4, in &lt;lambda&gt;\nNameError: name &#39;Rating&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2441)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2498)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor366.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;Rating&#39; is not defined&#39;, from &lt;command-1735170817424003&gt;, line 4. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/rddsampler.py&#34;, line 95, in func\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424003&gt;&#34;, line 4, in &lt;lambda&gt;\nNameError: name &#39;Rating&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 147) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;Rating&#39; is not defined&#39;, from &lt;command-1735170817424003&gt;, line 4. Full traceback below:","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424004&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>model <span class=\"ansi-blue-fg\">=</span> ALS<span class=\"ansi-blue-fg\">.</span>train<span class=\"ansi-blue-fg\">(</span>training_data<span class=\"ansi-blue-fg\">,</span>rank<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">,</span>iterations<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> testdata_no_rating <span class=\"ansi-blue-fg\">=</span> test_data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> p<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>p<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> predictions <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>predictAll<span class=\"ansi-blue-fg\">(</span>testdata_no_rating<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> predictions<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/mllib/recommendation.py</span> in <span class=\"ansi-cyan-fg\">train</span><span class=\"ansi-blue-fg\">(cls, ratings, rank, iterations, lambda_, blocks, nonnegative, seed)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    278</span>             <span class=\"ansi-blue-fg\">(</span>default<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    279</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 280</span><span class=\"ansi-red-fg\">         model = callMLlibFunc(&#34;trainALSModel&#34;, cls._prepare(ratings), rank, iterations,\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    281</span>                               lambda_, blocks, nonnegative, seed)\n<span class=\"ansi-green-intense-fg ansi-bold\">    282</span>         <span class=\"ansi-green-fg\">return</span> MatrixFactorizationModel<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/mllib/recommendation.py</span> in <span class=\"ansi-cyan-fg\">_prepare</span><span class=\"ansi-blue-fg\">(cls, ratings)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    232</span>             raise TypeError(&#34;Ratings should be represented by either an RDD or a DataFrame, &#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    233</span>                             &#34;but got %s.&#34; % type(ratings))\n<span class=\"ansi-green-fg\">--&gt; 234</span><span class=\"ansi-red-fg\">         </span>first <span class=\"ansi-blue-fg\">=</span> ratings<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    235</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>first<span class=\"ansi-blue-fg\">,</span> Rating<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    236</span>             <span class=\"ansi-green-fg\">pass</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1619</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1620</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1621</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1622</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1623</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1599</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1600</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1601</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1602</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1603</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1339</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1340</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1341</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1342</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 147) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;Rating&#39; is not defined&#39;, from &lt;command-1735170817424003&gt;, line 4. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/rddsampler.py&#34;, line 95, in func\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424003&gt;&#34;, line 4, in &lt;lambda&gt;\nNameError: name &#39;Rating&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2441)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2498)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor366.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;Rating&#39; is not defined&#39;, from &lt;command-1735170817424003&gt;, line 4. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/rddsampler.py&#34;, line 95, in func\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424003&gt;&#34;, line 4, in &lt;lambda&gt;\nNameError: name &#39;Rating&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2498)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\npreds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\nrates_and_preds = rates.join(preds)\nMSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\nprint(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9903f18e-fe56-4adf-9710-6b4e6a60f6e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424005&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> rates <span class=\"ansi-blue-fg\">=</span> ratings_final<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> r<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>preds <span class=\"ansi-blue-fg\">=</span> predictions<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> r<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> rates_and_preds <span class=\"ansi-blue-fg\">=</span> rates<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>preds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> MSE <span class=\"ansi-blue-fg\">=</span> rates_and_preds<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> r<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">-</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">**</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>mean<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Mean Squared Error of the model for the test data = {:.2f}&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>MSE<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;predictions&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;predictions&#39; is not defined","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424005&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> rates <span class=\"ansi-blue-fg\">=</span> ratings_final<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> r<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>preds <span class=\"ansi-blue-fg\">=</span> predictions<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> r<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> rates_and_preds <span class=\"ansi-blue-fg\">=</span> rates<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>preds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> MSE <span class=\"ansi-blue-fg\">=</span> rates_and_preds<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> r<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">-</span> r<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">**</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>mean<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Mean Squared Error of the model for the test data = {:.2f}&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>MSE<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;predictions&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_path_spam = \"/FileStore/tables/spam.txt\"\nfile_path_non_spam = \"/FileStore/tables/ham.txt\"\nspam_rdd = sc.textFile(file_path_spam)\nnon_spam_rdd = sc.textFile(file_path_non_spam)\n\nspam_words = spam_rdd.flatMap(lambda email: email.split(' '))\nnon_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n\nprint(\"The first element in spam_words is\", spam_words.first())\nprint(\"The first element in non_spam_words is\", non_spam_words.first())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22e6b8bb-e086-457b-9ed9-c4a13de3c234"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The first element in spam_words is You\nThe first element in non_spam_words is Rofl.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The first element in spam_words is You\nThe first element in non_spam_words is Rofl.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.feature import HashingTF\ntf = HashingTF(numFeatures=200)\nspam_features = tf.transform(spam_words)\nnon_spam_features = tf.transform(non_spam_words)\n\nspam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\nnon_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\nsamples = spam_samples.join(non_spam_samples)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae061965-5404-49d0-ae98-b702e738c1cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["train_samples,test_samples = samples.randomSplit([0.8, 0.2])\nmodel = LogisticRegressionWithLBFGS.train(train_samples)\npredictions = model.predict(test_samples.map(lambda x: x.features))\nlabels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\naccuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\nprint(\"Model accuracy : {:.2f}\".format(accuracy))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdeb0d16-4e97-4d3a-ad70-a6b946ccb762"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424008&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> train_samples<span class=\"ansi-blue-fg\">,</span>test_samples <span class=\"ansi-blue-fg\">=</span> samples<span class=\"ansi-blue-fg\">.</span>randomSplit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.8</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0.2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>model <span class=\"ansi-blue-fg\">=</span> LogisticRegressionWithLBFGS<span class=\"ansi-blue-fg\">.</span>train<span class=\"ansi-blue-fg\">(</span>train_samples<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> predictions <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>predict<span class=\"ansi-blue-fg\">(</span>test_samples<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>features<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> labels_and_preds <span class=\"ansi-blue-fg\">=</span> test_samples<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>label<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>zip<span class=\"ansi-blue-fg\">(</span>predictions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> accuracy <span class=\"ansi-blue-fg\">=</span> labels_and_preds<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">/</span> float<span class=\"ansi-blue-fg\">(</span>test_samples<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/mllib/classification.py</span> in <span class=\"ansi-cyan-fg\">train</span><span class=\"ansi-blue-fg\">(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    413</span>         <span class=\"ansi-green-fg\">if</span> initialWeights <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    414</span>             <span class=\"ansi-green-fg\">if</span> numClasses <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 415</span><span class=\"ansi-red-fg\">                 </span>initialWeights <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">*</span> len<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>features<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    416</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    417</span>                 <span class=\"ansi-green-fg\">if</span> intercept<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1619</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1620</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1621</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1622</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1623</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1599</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1600</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1601</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1602</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1603</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1339</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1340</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1341</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1342</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 84.0 failed 1 times, most recent failure: Lost task 3.0 in stage 84.0 (TID 153) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;LabeledPoint&#39; is not defined&#39;, from &lt;command-1735170817424007&gt;, line 7. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424007&gt;&#34;, line 7, in &lt;lambda&gt;\nNameError: name &#39;LabeledPoint&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:540)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2227)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:332)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2441)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2498)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor366.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;LabeledPoint&#39; is not defined&#39;, from &lt;command-1735170817424007&gt;, line 7. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424007&gt;&#34;, line 7, in &lt;lambda&gt;\nNameError: name &#39;LabeledPoint&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:540)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2227)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:332)\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 84.0 failed 1 times, most recent failure: Lost task 3.0 in stage 84.0 (TID 153) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;LabeledPoint&#39; is not defined&#39;, from &lt;command-1735170817424007&gt;, line 7. Full traceback below:","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424008&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> train_samples<span class=\"ansi-blue-fg\">,</span>test_samples <span class=\"ansi-blue-fg\">=</span> samples<span class=\"ansi-blue-fg\">.</span>randomSplit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.8</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0.2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>model <span class=\"ansi-blue-fg\">=</span> LogisticRegressionWithLBFGS<span class=\"ansi-blue-fg\">.</span>train<span class=\"ansi-blue-fg\">(</span>train_samples<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> predictions <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>predict<span class=\"ansi-blue-fg\">(</span>test_samples<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>features<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> labels_and_preds <span class=\"ansi-blue-fg\">=</span> test_samples<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>label<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>zip<span class=\"ansi-blue-fg\">(</span>predictions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> accuracy <span class=\"ansi-blue-fg\">=</span> labels_and_preds<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">/</span> float<span class=\"ansi-blue-fg\">(</span>test_samples<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/mllib/classification.py</span> in <span class=\"ansi-cyan-fg\">train</span><span class=\"ansi-blue-fg\">(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    413</span>         <span class=\"ansi-green-fg\">if</span> initialWeights <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    414</span>             <span class=\"ansi-green-fg\">if</span> numClasses <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 415</span><span class=\"ansi-red-fg\">                 </span>initialWeights <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">*</span> len<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>features<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    416</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    417</span>                 <span class=\"ansi-green-fg\">if</span> intercept<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1619</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1620</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1621</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1622</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1623</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1599</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1600</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1601</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1602</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1603</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1339</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1340</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1341</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1342</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 84.0 failed 1 times, most recent failure: Lost task 3.0 in stage 84.0 (TID 153) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;LabeledPoint&#39; is not defined&#39;, from &lt;command-1735170817424007&gt;, line 7. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424007&gt;&#34;, line 7, in &lt;lambda&gt;\nNameError: name &#39;LabeledPoint&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:540)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2227)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:332)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2441)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2498)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor366.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;LabeledPoint&#39; is not defined&#39;, from &lt;command-1735170817424007&gt;, line 7. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424007&gt;&#34;, line 7, in &lt;lambda&gt;\nNameError: name &#39;LabeledPoint&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:540)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2227)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:332)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["file_path = \"/FileStore/tables/5000_points.txt\"\nclusterRDD = sc.textFile(file_path)\nrdd_split = clusterRDD.map(lambda x: x.split('\\t'))\nrdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\nprint(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb788857-84af-4500-baa0-131e81d5da0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">There are 5000 rows in the rdd_split_int dataset\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">There are 5000 rows in the rdd_split_int dataset\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for clst in range(13, 17):\n  model = KMeans.train(rdd_split_int, clst, seed=1)\n  WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n  print(\"The cluster {} hasWithin Set Sum of Squared Error {}\".format(clst,WSSSE))\n\nmodel = KMeans.train(rdd_split_int, k=15, seed=1)\ncluster_centers = model.clusterCenters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77898af9-ed3f-448e-8c25-c945c924d94e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424010&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> clst <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">13</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">17</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   model <span class=\"ansi-blue-fg\">=</span> KMeans<span class=\"ansi-blue-fg\">.</span>train<span class=\"ansi-blue-fg\">(</span>rdd_split_int<span class=\"ansi-blue-fg\">,</span> clst<span class=\"ansi-blue-fg\">,</span> seed<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\">   </span>WSSSE <span class=\"ansi-blue-fg\">=</span> rdd_split_int<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> point<span class=\"ansi-blue-fg\">:</span> error<span class=\"ansi-blue-fg\">(</span>point<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>reduce<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">:</span> x <span class=\"ansi-blue-fg\">+</span> y<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;The cluster {} hasWithin Set Sum of Squared Error {}&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>clst<span class=\"ansi-blue-fg\">,</span>WSSSE<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">reduce</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1030</span>             <span class=\"ansi-green-fg\">yield</span> reduce<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> iterator<span class=\"ansi-blue-fg\">,</span> initial<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1031</span> \n<span class=\"ansi-green-fg\">-&gt; 1032</span><span class=\"ansi-red-fg\">         </span>vals <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>func<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1033</span>         <span class=\"ansi-green-fg\">if</span> vals<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1034</span>             <span class=\"ansi-green-fg\">return</span> reduce<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> vals<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    965</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    966</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 967</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    968</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    969</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 211) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;error&#39; is not defined&#39;, from &lt;command-1735170817424010&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1027, in func\n    initial = next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424010&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;error&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;error&#39; is not defined&#39;, from &lt;command-1735170817424010&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1027, in func\n    initial = next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424010&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;error&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 211) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;error&#39; is not defined&#39;, from &lt;command-1735170817424010&gt;, line 3. Full traceback below:","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424010&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> clst <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">13</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">17</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   model <span class=\"ansi-blue-fg\">=</span> KMeans<span class=\"ansi-blue-fg\">.</span>train<span class=\"ansi-blue-fg\">(</span>rdd_split_int<span class=\"ansi-blue-fg\">,</span> clst<span class=\"ansi-blue-fg\">,</span> seed<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\">   </span>WSSSE <span class=\"ansi-blue-fg\">=</span> rdd_split_int<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> point<span class=\"ansi-blue-fg\">:</span> error<span class=\"ansi-blue-fg\">(</span>point<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>reduce<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">:</span> x <span class=\"ansi-blue-fg\">+</span> y<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;The cluster {} hasWithin Set Sum of Squared Error {}&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>clst<span class=\"ansi-blue-fg\">,</span>WSSSE<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">reduce</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1030</span>             <span class=\"ansi-green-fg\">yield</span> reduce<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> iterator<span class=\"ansi-blue-fg\">,</span> initial<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1031</span> \n<span class=\"ansi-green-fg\">-&gt; 1032</span><span class=\"ansi-red-fg\">         </span>vals <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>func<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1033</span>         <span class=\"ansi-green-fg\">if</span> vals<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1034</span>             <span class=\"ansi-green-fg\">return</span> reduce<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> vals<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    965</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    966</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 967</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    968</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    969</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 211) (ip-10-172-193-240.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;error&#39; is not defined&#39;, from &lt;command-1735170817424010&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1027, in func\n    initial = next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424010&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;error&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2707)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2707)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1256)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2974)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2915)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2903)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;error&#39; is not defined&#39;, from &lt;command-1735170817424010&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 705, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1027, in func\n    initial = next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1735170817424010&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;error&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:148)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nrdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\nrdd_split_int_df_pandas = rdd_split_int_df.toPandas()\ncluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\nplt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\nplt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"], color=\"red\", marker=\"x\")\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"908fc79b-15b7-43ed-a1e3-c93f8d8ac56f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424011&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> rdd_split_int_df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>rdd_split_int<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> rdd_split_int_df_pandas <span class=\"ansi-blue-fg\">=</span> rdd_split_int_df<span class=\"ansi-blue-fg\">.</span>toPandas<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>cluster_centers_pandas <span class=\"ansi-blue-fg\">=</span> pd<span class=\"ansi-blue-fg\">.</span>DataFrame<span class=\"ansi-blue-fg\">(</span>cluster_centers<span class=\"ansi-blue-fg\">,</span> columns<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> color<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;red&#34;</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;x&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;cluster_centers&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;cluster_centers&#39; is not defined","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1735170817424011&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> rdd_split_int_df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>rdd_split_int<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> rdd_split_int_df_pandas <span class=\"ansi-blue-fg\">=</span> rdd_split_int_df<span class=\"ansi-blue-fg\">.</span>toPandas<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>cluster_centers_pandas <span class=\"ansi-blue-fg\">=</span> pd<span class=\"ansi-blue-fg\">.</span>DataFrame<span class=\"ansi-blue-fg\">(</span>cluster_centers<span class=\"ansi-blue-fg\">,</span> columns<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> rdd_split_int_df_pandas<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> color<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;red&#34;</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;x&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;cluster_centers&#39; is not defined</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-exe-5","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3519205206706791}},"nbformat":4,"nbformat_minor":0}
